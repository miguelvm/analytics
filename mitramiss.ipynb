{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining, Filtering, and Loading Relational Data with AWS Glue\n",
    "\n",
    "This example shows how to do joins and filters with transforms entirely on DynamicFrames.\n",
    "\n",
    "### 1. Crawl our sample dataset\n",
    "\n",
    "The dataset we'll be using in this example was downloaded from the [EveryPolitician](http://everypolitician.org)\n",
    "website into our sample-dataset bucket in S3, at:\n",
    "\n",
    "    s3://awsglue-datasets/examples/us-legislators.\n",
    "\n",
    "It contains data in JSON format about United States legislators and the seats they have held\n",
    "in the the House of Representatives and the Senate.\n",
    "\n",
    "For purposes of our example code, we are assuming that you have created an AWS S3 target bucket and folder,\n",
    "which we refer to here as `s3://glue-sample-target/output-dir/`.\n",
    "\n",
    "The first step is to crawl this data and put the results into a database called `legislators`\n",
    "in your Data Catalog, as described [here in the Developer Guide](http://docs.aws.amazon.com/glue/latest/dg/console-crawlers.html).\n",
    "The crawler will create the following tables in the `legislators` database:\n",
    "\n",
    " - `persons_json`\n",
    " - `memberships_json`\n",
    " - `organizations_json`\n",
    " - `events_json`\n",
    " - `areas_json`\n",
    " - `countries_r_json`\n",
    "\n",
    "This is a semi-normalized collection of tables containing legislators and their histories.\n",
    "\n",
    "### 2. Getting started\n",
    "\n",
    "We will write a script that:\n",
    "\n",
    "1. Combines persons, organizations, and membership histories into a single legislator\n",
    "   history data set. This is often referred to as de-normalization.\n",
    "2. Separates out the senators from the representatives.\n",
    "3. Writes each of these out to separate parquet files for later analysis.\n",
    "\n",
    "Begin by running some boilerplate to import the AWS Glue libraries we'll need and set up a single `GlueContext`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1595925451260_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-42-178.eu-west-1.compute.internal:20888/proxy/application_1595925451260_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-45-158.eu-west-1.compute.internal:8042/node/containerlogs/container_1595925451260_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2edaab563714777a8523b8fb5a29aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c531c0b1f3491992464437e22dddb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Checking the schemas that the crawler identified\n",
    "\n",
    "Next, you can easily examine the schemas that the crawler recorded in the Data Catalog. For example,\n",
    "to see the schema of the `persons_json` table, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c344703c88554a5aad7d52ed4392bd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o93.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 0.0 failed 4 times, most recent failure: Lost task 10.3 in stage 0.0 (TID 29, ip-172-31-40-144.eu-west-1.compute.internal, executor 1): com.amazonaws.services.glue.util.FatalException: Unable to parse file: part-00003-e4690a10-6782-470c-b9d1-38b0b46d0fa5-c000.snappy.parquet\n",
      "\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNextFailSafe(JacksonReader.scala:94)\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNext(JacksonReader.scala:38)\n",
      "\tat com.amazonaws.services.glue.hadoop.TapeHadoopRecordReaderSplittable.nextKeyValue(TapeHadoopRecordReaderSplittable.scala:97)\n",
      "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:230)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n",
      "\tat com.amazonaws.services.glue.DynamicFrame.count(DynamicFrame.scala:1146)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.amazonaws.services.glue.util.FatalException: Unable to parse file: part-00003-e4690a10-6782-470c-b9d1-38b0b46d0fa5-c000.snappy.parquet\n",
      "\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNextFailSafe(JacksonReader.scala:94)\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNext(JacksonReader.scala:38)\n",
      "\tat com.amazonaws.services.glue.hadoop.TapeHadoopRecordReaderSplittable.nextKeyValue(TapeHadoopRecordReaderSplittable.scala:97)\n",
      "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:230)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1595925451260_0002/container_1595925451260_0002_01_000001/PyGlue.zip/awsglue/dynamicframe.py\", line 294, in count\n",
      "    return self._jdf.count()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1595925451260_0002/container_1595925451260_0002_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1595925451260_0002/container_1595925451260_0002_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1595925451260_0002/container_1595925451260_0002_01_000001/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o93.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 0.0 failed 4 times, most recent failure: Lost task 10.3 in stage 0.0 (TID 29, ip-172-31-40-144.eu-west-1.compute.internal, executor 1): com.amazonaws.services.glue.util.FatalException: Unable to parse file: part-00003-e4690a10-6782-470c-b9d1-38b0b46d0fa5-c000.snappy.parquet\n",
      "\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNextFailSafe(JacksonReader.scala:94)\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNext(JacksonReader.scala:38)\n",
      "\tat com.amazonaws.services.glue.hadoop.TapeHadoopRecordReaderSplittable.nextKeyValue(TapeHadoopRecordReaderSplittable.scala:97)\n",
      "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:230)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n",
      "\tat com.amazonaws.services.glue.DynamicFrame.count(DynamicFrame.scala:1146)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: com.amazonaws.services.glue.util.FatalException: Unable to parse file: part-00003-e4690a10-6782-470c-b9d1-38b0b46d0fa5-c000.snappy.parquet\n",
      "\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNextFailSafe(JacksonReader.scala:94)\n",
      "\tat com.amazonaws.services.glue.readers.JacksonReader.hasNext(JacksonReader.scala:38)\n",
      "\tat com.amazonaws.services.glue.hadoop.TapeHadoopRecordReaderSplittable.nextKeyValue(TapeHadoopRecordReaderSplittable.scala:97)\n",
      "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:230)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons = glueContext.create_dynamic_frame.from_catalog(database=\"epa\", table_name=\"echpersonas_string\")\n",
    "print(\"Count: \" + str(persons.count()))\n",
    "persons.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "applymapping1 = ApplyMapping.apply(frame = persons, mappings = [(\"id_viv\", \"string\", \"id_viv\", \"string\"), (\"npv\", \"string\", \"npv\", \"string\"), (\"periodo\", \"string\", \"periodo\", \"string\"), (\"tamano\", \"string\", \"tamano\", \"string\"), (\"idq_pv\", \"string\", \"idq_pv\", \"string\"), (\"ca\", \"string\", \"ca\", \"string\"), (\"faccal\", \"string\", \"faccal\", \"decimal(10,2)\"), (\"sexo\", \"string\", \"sexo\", \"string\"), (\"edad\", \"string\", \"edad\", \"string\"), (\"ec\", \"string\", \"ec\", \"string\"), (\"nacim\", \"string\", \"nacim\", \"string\"), (\"pnacimt\", \"string\", \"pnacimt\", \"string\"), (\"edadflleg\", \"string\", \"edadflleg\", \"string\"), (\"tpoflleg\", \"string\", \"tpoflleg\", \"int\"), (\"nac\", \"string\", \"nac\", \"string\"), (\"pnact\", \"string\", \"pnact\", \"string\"), (\"nacnacimesp\", \"string\", \"nacnacimesp\", \"string\"), (\"pnacnacimt\", \"string\", \"pnacnacimt\", \"string\"), (\"tpofnacesp\", \"string\", \"tpofnacesp\", \"int\"), (\"nacimpadre\", \"string\", \"nacimpadre\", \"string\"), (\"pnacimpadret\", \"string\", \"pnacimpadret\", \"string\"), (\"nacimmadre\", \"string\", \"nacimmadre\", \"string\"), (\"pnacimmadret\", \"string\", \"pnacimmadret\", \"string\"), (\"p01\", \"string\", \"p01\", \"string\"), (\"p02\", \"string\", \"p02\", \"string\"), (\"p03\", \"string\", \"p03\", \"string\"), (\"p04\", \"string\", \"p04\", \"string\"), (\"p05\", \"string\", \"p05\", \"string\"), (\"p06\", \"string\", \"p06\", \"string\"), (\"p07\", \"string\", \"p07\", \"string\"), (\"p08\", \"string\", \"p08\", \"string\"), (\"p09\", \"string\", \"p09\", \"string\"), (\"p10\", \"string\", \"p10\", \"string\"), (\"p11\", \"string\", \"p11\", \"string\"), (\"p12\", \"string\", \"p12\", \"string\"), (\"p13\", \"string\", \"p13\", \"string\"), (\"p14\", \"string\", \"p14\", \"string\"), (\"p15\", \"string\", \"p15\", \"string\"), (\"p16\", \"string\", \"p16\", \"string\"), (\"p17\", \"string\", \"p17\", \"string\"), (\"p18\", \"string\", \"p18\", \"string\"), (\"p19\", \"string\", \"p19\", \"string\"), (\"estudios\", \"string\", \"estudios\", \"string\"), (\"relact\", \"string\", \"relact\", \"string\"), (\"ocupa\", \"string\", \"ocupa\", \"string\"), (\"situho\", \"string\", \"situho\", \"string\"), (\"situho_d\", \"string\", \"situho_d\", \"string\"), (\"pareja\", \"string\", \"pareja\", \"string\"), (\"sexopar\", \"string\", \"sexopar\", \"string\"), (\"nacpar\", \"string\", \"nacpar\", \"string\"), (\"ecpar\", \"string\", \"ecpar\", \"string\"), (\"nhijo_nucleo\", \"string\", \"nhijo_nucleo\", \"int\"), (\"nhijome_nucleo\", \"string\", \"nhijome_nucleo\", \"int\"), (\"hijosdeambos\", \"string\", \"hijosdeambos\", \"string\"), (\"situnucleofam\", \"string\", \"situnucleofam\", \"string\")], transformation_ctx = \"applymapping1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = \"make_struct\", transformation_ctx = \"resolvechoice2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null_fields []"
     ]
    }
   ],
   "source": [
    "dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = \"dropnullfields3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = \"s3\", connection_options = {\"path\": \"s3://mitramiss/ECHPersonas\"}, format = \"parquet\", transformation_ctx = \"datasink4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each person in the table is a member of some congressional body.\n",
    "\n",
    "Look at the schema of the `memberships_json` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships = glueContext.create_dynamic_frame.from_catalog(database=\"legislators\", table_name=\"memberships_json\")\n",
    "print(\"Count: \" + str(memberships.count()))\n",
    "memberships.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizations are parties and the two chambers of congress, the Senate and House.\n",
    "Look at the schema of the `organizations_json` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = glueContext.create_dynamic_frame.from_catalog(database=\"legislators\", table_name=\"organizations_json\")\n",
    "print(\"Count: \" + str(orgs.count()))\n",
    "orgs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filtering\n",
    "\n",
    "Let's only keep the fields that we want and rename `id` to `org_id`. The dataset is small enough that we can\n",
    "look at the whole thing. The `toDF()` converts a DynamicFrame to a Spark DataFrame, so we can apply the\n",
    "transforms that already exist in SparkSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = orgs.drop_fields(['other_names','identifiers']).rename_field('id', 'org_id').rename_field('name', 'org_name')\n",
    "orgs.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `organizations` that appear in `memberships`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships.select_fields(['organization_id']).toDF().distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Putting it together\n",
    "\n",
    "Now let's join these relational tables to create one full history table of legislator\n",
    "memberships and their correponding organizations, using AWS Glue.\n",
    "\n",
    " - First, we join `persons` and `memberships` on `id` and `person_id`.\n",
    " - Next, join the result with orgs on `org_id` and `organization_id`.\n",
    " - Then, drop the redundant fields, `person_id` and `org_id`.\n",
    "\n",
    "We can do all these operations in one (extended) line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_history = Join.apply(orgs,\n",
    "                       Join.apply(persons, memberships, 'id', 'person_id'),\n",
    "                       'org_id', 'organization_id').drop_fields(['person_id', 'org_id'])\n",
    "print(\"Count: \" + str(l_history.count()))\n",
    "l_history.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have the final table that we'd like to use for analysis.\n",
    "Let's write it out in a compact, efficient format for analytics, i.e. Parquet,\n",
    "that we can run SQL over in AWS Glue, Athena, or Redshift Spectrum.\n",
    "\n",
    "The following call writes the table across multiple files to support fast parallel\n",
    "reads when doing analysis later:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glueContext.write_dynamic_frame.from_options(frame = l_history,\n",
    "              connection_type = \"s3\",\n",
    "              connection_options = {\"path\": \"s3://glue-sample-target/output-dir/legislator_history\"},\n",
    "              format = \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put all the history data into a single file, we need to convert it to a data frame, repartition it, and\n",
    "write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_history = l_history.toDF().repartition(1)\n",
    "s_history.write.parquet('s3://glue-sample-target/output-dir/legislator_single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to separate it by the Senate and the House:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_history.toDF().write.parquet('s3://glue-sample-target/output-dir/legislator_part', partitionBy=['org_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Writing to Relational Databases\n",
    "\n",
    "AWS Glue makes it easy to write it to relational databases like Redshift even with\n",
    "semi-structured data. It offers a transform, `relationalize()`, that flattens DynamicFrames\n",
    "no matter how complex the objects in the frame may be.\n",
    "\n",
    "Using the `l_history` DynamicFrame in our example, we pass in the name of a root table\n",
    "(`hist_root`) and a temporary working path to `relationalize`, which returns a `DynamicFrameCollection`.\n",
    "We then list the names of the DynamicFrames in that collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = l_history.relationalize(\"hist_root\", \"s3://glue-sample-target/temp-dir/\")\n",
    "dfc.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationalize broke the history table out into 6 new tables: a root table containing a record for each object in the\n",
    "dynamic frame, and auxiliary tables for the arrays. Array handling in relational databases is often sub-optimal,\n",
    "especially as those arrays become large. Separating out the arrays into separate tables makes the queries go much\n",
    "faster.\n",
    "\n",
    "Let's take a look at the separation by examining `contact_details`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_history.select_fields('contact_details').printSchema()\n",
    "dfc.select('hist_root_contact_details').toDF().where(\"id = 10 or id = 75\").orderBy(['id','index']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contact_details` field was an array of structs in the original DynamicFrame.\n",
    "Each element of those arrays is a separate row in the auxiliary table, indexed by\n",
    "`index`. The `id` here is a foreign key into the `hist_root` table with the key\n",
    "`contact_details`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.select('hist_root').toDF().where(\"contact_details = 10 or contact_details = 75\").select(['id', 'given_name', 'family_name', 'contact_details']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the commands above that we used `toDF()` and subsequently a `where` expression to filter for the rows that\n",
    "we wanted to see.\n",
    "\n",
    "So, joining the `hist_root` table with the auxiliary tables allows you to:\n",
    "\n",
    " - Load data into databases without array support.\n",
    " - Query each individual item in an array using SQL.\n",
    "\n",
    "We already have a connection set up called `redshift3`. To create your own, see\n",
    "[this topic in the Developer Guide](http://docs.aws.amazon.com/glue/latest/dg/populate-add-connection.html).\n",
    "Let's write this collection into Redshift by cycling through the DynamicFrames one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in dfc.keys():\n",
    "        m_df = dfc.select(df_name)\n",
    "        print(\"Writing to Redshift table: \" + df_name)\n",
    "        glueContext.write_dynamic_frame.from_jdbc_conf(frame = m_df,\n",
    "                                                       catalog_connection = \"redshift3\",\n",
    "                                                       connection_options = {\"dbtable\": df_name, \"database\": \"testdb\"},\n",
    "                                                       redshift_tmp_dir = \"s3://glue-sample-target/temp-dir/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the commands above that we used `toDF()` and subsequently a `where` expression to filter for the rows that\n",
    "we wanted to see.\n",
    "\n",
    "So, joining the `hist_root` table with the auxiliary tables allows you to:\n",
    "\n",
    " - Load data into databases without array support.\n",
    " - Query each individual item in an array using SQL.\n",
    "\n",
    "We already have a connection set up called `redshift3`. To create your own, see\n",
    "[this topic in the Developer Guide](http://docs.aws.amazon.com/glue/latest/dg/populate-add-connection.html).\n",
    "Let's write this collection into Redshift by cycling through the DynamicFrames one at a time:\n",
    "\n",
    "    for df_name in dfc.keys():\n",
    "        m_df = dfc.select(df_name)\n",
    "        print(\"Writing to Redshift table: \" + df_name)\n",
    "        glueContext.write_dynamic_frame.from_jdbc_conf(frame = m_df,\n",
    "                                                       catalog_connection = \"redshift3\",\n",
    "                                                       connection_options = {\"dbtable\": df_name, \"database\": \"testdb\"},\n",
    "                                                       redshift_tmp_dir = \"s3://glue-sample-target/temp-dir/\")\n",
    "\n",
    "Here's what the tables look like in Redshift. (We connected to Redshift through psql.)\n",
    "\n",
    "    testdb=# \\d\n",
    "                       List of relations\n",
    "     schema |           name            | type  |   owner\n",
    "    --------+---------------------------+-------+-----------\n",
    "     public | hist_root                 | table | test_user\n",
    "     public | hist_root_contact_details | table | test_user\n",
    "     public | hist_root_identifiers     | table | test_user\n",
    "     public | hist_root_images          | table | test_user\n",
    "     public | hist_root_links           | table | test_user\n",
    "     public | hist_root_other_names     | table | test_user\n",
    "    (6 rows)\n",
    "\n",
    "    testdb=# \\d hist_root_contact_details\n",
    "                 Table \"public.hist_root_contact_details\"\n",
    "              Column           |           Type           | Modifiers\n",
    "    ---------------------------+--------------------------+-----------\n",
    "     id                        | bigint                   |\n",
    "     index                     | integer                  |\n",
    "     contact_details.val.type  | character varying(65535) |\n",
    "     contact_details.val.value | character varying(65535) |\n",
    "\n",
    "    testdb=# \\d hist_root\n",
    "                       Table \"public.hist_root\"\n",
    "            Column         |           Type           | Modifiers\n",
    "    -----------------------+--------------------------+-----------\n",
    "     role                  | character varying(65535) |\n",
    "     seats                 | integer                  |\n",
    "     org_name              | character varying(65535) |\n",
    "     links                 | bigint                   |\n",
    "     type                  | character varying(65535) |\n",
    "     sort_name             | character varying(65535) |\n",
    "     area_id               | character varying(65535) |\n",
    "     images                | bigint                   |\n",
    "     on_behalf_of_id       | character varying(65535) |\n",
    "     other_names           | bigint                   |\n",
    "     birth_date            | character varying(65535) |\n",
    "     name                  | character varying(65535) |\n",
    "     organization_id       | character varying(65535) |\n",
    "     gender                | character varying(65535) |\n",
    "     classification        | character varying(65535) |\n",
    "     legislative_period_id | character varying(65535) |\n",
    "     identifiers           | bigint                   |\n",
    "     given_name            | character varying(65535) |\n",
    "     image                 | character varying(65535) |\n",
    "     family_name           | character varying(65535) |\n",
    "     id                    | character varying(65535) |\n",
    "     death_date            | character varying(65535) |\n",
    "     start_date            | character varying(65535) |\n",
    "     contact_details       | bigint                   |\n",
    "     end_date              | character varying(65535) |\n",
    "\n",
    "Now you can query these tables using SQL in Redshift:\n",
    "\n",
    "    testdb=# select * from hist_root_contact_details where id = 10 or id = 75 order by id, index;\n",
    "\n",
    "With this result:\n",
    "\n",
    "     id | index | contact_details.val.type | contact_details.val.value\n",
    "    ----+-------+--------------------------+---------------------------\n",
    "     10 |     0 | fax                      |\n",
    "     10 |     1 |                          | 202-225-1314\n",
    "     10 |     2 | phone                    |\n",
    "     10 |     3 |                          | 202-225-3772\n",
    "     10 |     4 | twitter                  |\n",
    "     10 |     5 |                          | MikeRossUpdates\n",
    "     75 |     0 | fax                      |\n",
    "     75 |     1 |                          | 202-225-7856\n",
    "     75 |     2 | phone                    |\n",
    "     75 |     3 |                          | 202-225-2711\n",
    "     75 |     4 | twitter                  |\n",
    "     75 |     5 |                          | SenCapito\n",
    "    (12 rows)\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Overall, AWS Glue is quite flexible allowing you to do in a few lines of code, what normally would take days to\n",
    "write. The entire source to target ETL scripts from end-to-end can be found in the accompanying Python file,\n",
    "[join_and_relationalize.py](join_and_relationalize.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
